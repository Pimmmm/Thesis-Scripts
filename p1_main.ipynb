{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSc thesis part 1: main script\n",
    "\n",
    "### Several functions have been defined and are saved in separate files\n",
    "### Author: Pim Arendsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import nbimporter\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import utils, transforms, models\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "#import functions from other files\n",
    "from p1_CreateTrainingDataframe import CreateTrainingDataframe\n",
    "from p1_CreateTestDataframe import CreateTestDataframe\n",
    "from p1_BrodenDataSet import BrodenDataset\n",
    "from p1_RescaleImage import Rescale\n",
    "from p1_TransformToTensor import ToTensor\n",
    "from p1_GetVectorFromImage import GetVector\n",
    "from p1_GetCosineSimilarityDistance import GetCosineSimilarityDistance\n",
    "from p1_MakeVectorDictionary import MakeVectorDictionary\n",
    "from p1_SubsetConceptImages import SubsetConceptImages\n",
    "from p1_GetCAV import GetCav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed to ensure similar randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Download a pretrained ResNet50 and edit the last layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a pretrained model and define the layer from which to extract the image vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "basenet = models.resnet50(pretrained=True)\n",
    "out_layer = basenet._modules.get('avgpool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Define the concepts in the Broden dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define directories of the different folders. \n",
    "Set the working directory and change the directories accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "broden_dataset_path = '../data/broden1_384/'\n",
    "index_file_path = os.path.join(broden_dataset_path, 'index.csv')\n",
    "label_file_path = os.path.join(broden_dataset_path, 'label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training dataset by parsing through the Broden dataset. The data is already split in 70% train and 30% test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REBUILD_DATA = False #set a manual flag to prevent rebuilding the data once it is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REBUILD_DATA:\n",
    "    \n",
    "    # create the training data dataframe\n",
    "    print('creating training dataset')\n",
    "    training_data = CreateTrainingDataframe(index_file_path, label_file_path, broden_dataset_path)\n",
    "    print('writing training data...')    \n",
    "    training_data.to_csv('../data/training_data.csv', index=False)\n",
    "    \n",
    "    # create the testdata dataframe\n",
    "    test_data = CreateTestDataframe(index_file_path, label_file_path, broden_dataset_path)\n",
    "    print('writing test data...')\n",
    "    test_data.to_csv('../data/test_data.csv', index=False)\n",
    "    print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training and test data is already available: update the following directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = '../data/training_data.csv'\n",
    "test_data_path = '../data/test_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3 Determine CAVs using the Broden dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Broden dataset is converted into a PyTorch Dataset class, which can be used to access indivual images. <br>\n",
    "The images are rescaled to 224x224 pixels, transformed to tensors and normalized according to the PyTorch documentation. <br>\n",
    "\n",
    "The dataset returns a dictionary with the following keys: \n",
    "- 'image': as ndarray or tensor, depending on the transformation\n",
    "- 'concept': a list of all the concepts within an image\n",
    "- 'concept_vector': a one-hot array for each concept in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_broden_dataset = BrodenDataset(csv_file = training_data_path, \n",
    "                               data_path = broden_dataset_path, \n",
    "                               transform = transforms.Compose([Rescale(224),\n",
    "                                                              ToTensor(),\n",
    "                                                               transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                    std=[0.229, 0.224, 0.225])\n",
    "                                                              ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_broden_dataset = BrodenDataset(csv_file = test_data_path,\n",
    "                                    data_path = broden_dataset_path, \n",
    "                                    transform = transforms.Compose([Rescale(224),\n",
    "                                                                    ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                         std=[0.229, 0.224, 0.225])\n",
    "                                                                   ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a concept activation vector will be determined for 6 concept. This will be expanded further on in the project to multiple concepts. This requires several steps:\n",
    "- get images for a specific concept\n",
    "- get random images which do not contain the concept\n",
    "- train a linear classifier between the concept and the random counter examples\n",
    "- determine the vector orthogonal to the linear classifier in the direction of the concept, this is the CAV\n",
    "\n",
    "**Questions to answers:**\n",
    "- How to cope with the imbalanced dataset? Will this influence the concepts or the counter examples?\n",
    "- What about miss classification of concepts in images?\n",
    "- How many images (concept images as well as counter examples) are needed to get an accurate CAV?\n",
    "- Do the amount of images need to be equal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(training_data_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get images for a specific concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'mountain'\n",
    "file_name = concept + '_vectors.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the row index values of the concept in the training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = training_data.loc[training_data[concept] == 1, 'image'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The concept \"%s\" is present in %d images' % (concept, len(idxs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first *n* images labelled with the above defined concept can be viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_IMG = False # set a manual flag to show images, this is done to run the entire script without have to load the images\n",
    "n = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHOW_IMG:\n",
    "    \n",
    "    for i in range(n):\n",
    "        print('row index', idxs[i])\n",
    "        plt.figure()\n",
    "        plt.imshow(io.imread(os.path.join(broden_dataset_path, 'images/', training_data.loc[idxs[i], 'image'])))\n",
    "        plt.show()\n",
    "        print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen by viewing some images, the labelled images are noisy, not all images contain the defined concept. As such, the images need to be filtered. This is done by applying the following steps:\n",
    "\n",
    "1. Push all images through the CNN and store the output tensors\n",
    "2. Manually, select 5 images from the Broden dataset which do contain the defined concept, referred to as *reference images*\n",
    "3. For each of these reference images: <br>\n",
    "    a. calculate the cosine similarity of the *reference tensor* to all other tensors <br>\n",
    "    b. select the tensors which fall within the similarity threshold \n",
    "4. The tensors which fall within the similarity threshold in multiple reference images contain the concept\n",
    "5. Link the selected tensors back to the image\n",
    "\n",
    "This will be explained in further detail below\n",
    "\n",
    "The method cannot be used to get the CAV of a concept, it should contain more randomness as the manual selection of images introduces bias. However, this methods can be used to show that the ResNet50 model captures semantical information. Which confirms the main assumption that images showing the same concept, result in similar tensors when run through the network. Or in other words, the models captures semantically similar images\n",
    "\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images from the Broden Dataset which are labelled with the defined concept are run through the model and are stored in a dictionary. The dictionary contains the following keys and values:\n",
    "- keys (string): row index of image in the training dataframe\n",
    "- value (torch.tensor): the vector from the *out_layer* after the image is passed through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKE_DICT = False # set a manual flag to prevent recreating the tensor dictionary if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAKE_DICT:\n",
    "    vector_dict = MakeVectorDictionary(basenet, out_layer, train_broden_dataset, idxs, file_name)\n",
    "else:\n",
    "    with open(os.path.join('../data/', file_name), 'rb') as handle:\n",
    "        vector_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For six concepts, 5 reference images have been manually defined. *This does cause confirmation bias. How can 5 images capture a concept? E.g. what exactly is a mountain? Where lies the difference between a mountain and a hill?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_reference_dict = {'mountain': ['6636', '33439', '30055', '21198', '33075'],\n",
    "                     'highway-s': ['3617', '2836', '407', '1013', '1594'],\n",
    "                     'river': ['1052', '2103', '3906', '4783', '1669'],\n",
    "                     'forest-broadleaf-s': ['2681', '4622', '6639', '15993', '12546'],\n",
    "                     'building': ['65', '109', '157', '61', '136'],\n",
    "                     'truck': ['95', '4166', '5886', '6270', '6874']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one of the concepts, the tensor of each of the reference images is used to calculate the cosine similarity with all other tensors. The cosine similarity calculates the angle between tensors in the feature space. Tensors which contain the concept should be similar to the reference tensor and thus have a small angle. The cosine similarity ranges from 1 to -1, where a value of 1 exactly matches the reference tensor and -1 goes in the opposite direction. This is where the *similarity threshold* is introduced. It determines how similar a tensor must be to the reference tensor. Tensors ranging from the similarity threshold - 1 are assumed to contain the concept and are stored. This is done for each of the reference tensors. If tensors fall within the similarity threshold multiple times (for different reference tensors), they are also stored multiple times.<br> \n",
    "\n",
    "This is where an other *overlap threshold* is introduced. As tensors can be within the cosine similarity threshold multiple times, the *overlap threshold* determines how many times the tensor must be stored. If the overlap is set to 5, only tensors which fall within the similarity threshold for each of the 5 reference images are selected. <br>\n",
    "\n",
    "The tensors which fall within both thresholds are linked back to the image indices.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_threshold = 0.75\n",
    "overlap_threshold = [4,5] \n",
    "\n",
    "concept_imgs = SubsetConceptImages(img_reference_dict, concept, vector_dict, similarity_threshold, overlap_threshold)\n",
    "print('Amount of filtered images with concept %s: %d' % (concept, len(concept_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.figure()\n",
    "    plt.imshow(io.imread(os.path.join(broden_dataset_path, 'images/', training_data.loc[int(concept_imgs[i]), 'image'])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a linear classifier between the concept images and the random images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To experiment with CAVs the 'scene' concepts are used, as these images are better classified then other concepts. The following scene concepts have been used to experiment with the different option to get a CAV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scene_concepts =['street-s', 'mountain_snowy-s', 'highway-s', 'skyscraper-s',  'park-s', 'mountain-s', 'coast-s',\n",
    "                'beach-s', 'field-cultivated-s', 'valley-s', 'bus_shelter-s', 'hayfield-s', 'hill-s', 'building_facade-s', \n",
    "                'pasture-s', 'apartment_building-outdoor-s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the amount of images representing a concept needs to be chosen. Some concepts have many images describing the concept. However, concepts like *bus shelter* only have 10 images to train a CAV on. These different amounts of images will be used to check how many images are required to train a accurate CAV.<br>\n",
    "For most concepts the ratio is *50 concept images : 100 random images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "concept = 'street-s'\n",
    "\n",
    "#concept images\n",
    "n = 50\n",
    "\n",
    "#random images\n",
    "m = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The CAV is calculated by randomly selected the indicated amount of concept images and random images. The images used are returned, just as the CAV and the linear classification model. This is used to check the accuracy <br>\n",
    "\n",
    "2. To test the accuracy of the model and thus the CAV, test images are used to check the prediction. To do this, for every *scene* concept, 50 random labelled test images are selected. If there are not sufficient test images, the total amount of test images is taken. All indices are merged into the same list, and they are added to a dictionary with the concept as key and the matching image indices as values\n",
    "3. The selected test images are run through the ResNet50 model and the tensors are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "\n",
    "for ii in range(20):\n",
    "    for n in range(10, 200, 10):\n",
    "\n",
    "        # 1. Train the CAV and the linear classification model\n",
    "        concept_idxs, counter_idxs, cav, model = GetCav(concept, n, m, training_data, basenet, out_layer, train_broden_dataset)\n",
    "\n",
    "\n",
    "        # 2. Indices of test data are stored\n",
    "        random.seed(123)\n",
    "        test_scene_dict = {}\n",
    "        test_scene_idxs =[]\n",
    "        for scene in scene_concepts:\n",
    "            scene_idx = test_data.loc[test_data[scene] == 1, 'image'].index.tolist()\n",
    "            if len(scene_idx) > 50:\n",
    "                scene_idx = random.sample(scene_idx, 50)\n",
    "\n",
    "            test_scene_dict[scene] = scene_idx\n",
    "\n",
    "            list(map(lambda x: test_scene_idxs.append(x), scene_idx))\n",
    "\n",
    "\n",
    "        # 3. Tensors for each image is extracted\n",
    "        test_filename = 'test_scene_vector.pickle'\n",
    "\n",
    "        test_scene_vector_dict = {}\n",
    "\n",
    "        if not os.path.exists(os.path.join('../data/', test_filename)):\n",
    "            for key in scene_concepts:\n",
    "                print(key)\n",
    "\n",
    "                # get all indices for the concept\n",
    "                temp_scene_idxs = test_scene_dict[key]\n",
    "\n",
    "                #for every scene, store the index value of the image as key and the tensor as value\n",
    "                temp_dict = MakeVectorDictionary(basenet, out_layer, test_broden_dataset, temp_scene_idxs, None)\n",
    "                temp_keys = list(temp_dict.keys())\n",
    "                test_scene_vector_dict[key] = temp_dict[temp_keys[0]].unsqueeze(0)\n",
    "\n",
    "                for i in range(1, len(temp_dict.keys())):\n",
    "                    #for every key (index value of image belonging to the concept), get the tensor and add it to the vector dictionary\n",
    "                    test_scene_vector_dict[key] = torch.cat((test_scene_vector_dict[key], temp_dict[temp_keys[i]].unsqueeze(0)))\n",
    "\n",
    "            with open(os.path.join('../data/', test_filename), 'wb') as handle:\n",
    "                    pickle.dump(test_scene_vector_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        else:\n",
    "            with open(os.path.join('../data/', test_filename), 'rb') as handle:\n",
    "                test_scene_vector_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "        # 4. Preprocess test data       \n",
    "        start_idx = 0\n",
    "        for key in scene_concepts:\n",
    "            if key == concept:\n",
    "                break\n",
    "            else:\n",
    "                start_idx += len(test_scene_vector_dict[key])\n",
    "\n",
    "        end_idx = start_idx + len(test_scene_vector_dict[concept])+1\n",
    "\n",
    "        #create the right labels on the right tensors\n",
    "        y_test = np.zeros(len(test_scene_idxs))\n",
    "        y_test[start_idx:end_idx] = 1\n",
    "\n",
    "        #create test matrix\n",
    "        X_test = test_scene_vector_dict[scene_concepts[0]]\n",
    "        for i in range(1, len(scene_concepts)):\n",
    "            temp_vector = test_scene_vector_dict[scene_concepts[i]]\n",
    "            X_test = torch.cat((X_test, temp_vector),0)\n",
    "\n",
    "        X_test = X_test.numpy()\n",
    "\n",
    "        # 5. Use the model to classify the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        acc = [n, m, [accuracy]]\n",
    "    \n",
    "        if ii > 0:\n",
    "            for sublist in accuracy_list:\n",
    "                if sublist[1] == n:  ####################### CHANGE THIS TO 'm' or 'n'\n",
    "                    sublist[2].append(accuracy)\n",
    "        else:\n",
    "            accuracy_list.append(acc)    \n",
    "    \n",
    "# 6. Plot accuracy with different parameters\n",
    "ns = []\n",
    "ms = []\n",
    "ys = []\n",
    "for sublist in accuracy_list:\n",
    "    ns.append(sublist[0])\n",
    "    ms.append(sublist[1])\n",
    "    ys.append(mean(sublist[2]))\n",
    "\n",
    "plt.plot(ns, ys)\n",
    "plt.ylim(0.4,1)\n",
    "plt.title(concept + '; counter examples: '+ str(m))\n",
    "plt.xlabel('amount of concept images')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchCAV class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCAV(object):\n",
    "    \n",
    "    def __init__(self, concept, num_concept_imgs, num_counter_imgs,\n",
    "                 n_network, output_layer, training_torch_dataset, test_torch_dataset, \n",
    "                 training_dataframe, test_dataframe, scene_concepts, training_mode = True):\n",
    "\n",
    "        self.concept = concept\n",
    "        self.num_concept_imgs = num_concept_imgs\n",
    "        self.num_counter_imgs = num_counter_imgs\n",
    "        self.n_network = n_network\n",
    "        self.output_layer = output_layer\n",
    "        self.training_torch_dataset = training_torch_dataset\n",
    "        self.test_torch_dataset = test_torch_dataset\n",
    "        self.training_dataframe = training_dataframe\n",
    "        self.test_dataframe = test_dataframe\n",
    "        self.training_mode = training_mode\n",
    "        self.scene_concepts = scene_concepts\n",
    "        \n",
    "        \n",
    "        self.concept_idxs = []\n",
    "        self.concept_dictionary = {}\n",
    "        self.random_concept_idxs = []\n",
    "        self.random_counter_idxs = []\n",
    "        self.counter_dictionary = {}\n",
    "        self.counter_dictionary_name = concept + '_' + str(num_counter_imgs) + '_counter_imgs.pickle'\n",
    "        self.test_scene_dict = {}\n",
    "        self.test_scene_idxs =[]\n",
    "        self.test_scene_vector_dictionary = {}\n",
    "        \n",
    "        self.lm = None\n",
    "        self.cav = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        if self.training_mode:\n",
    "            self.dictionary_name = concept + '_training_vectors.pickle'\n",
    "        else:\n",
    "            self.dictionary_name = concept + '_test_vectors.pickle'\n",
    "\n",
    "        if self.concept not in list(self.training_dataframe):\n",
    "            raise ValueError ('%s is not present in the dataset' % self.concept)\n",
    "\n",
    "        random.seed(42)    \n",
    "        \n",
    "    def get_concept_indices(self):\n",
    "        \n",
    "        '''\n",
    "        Get all the row indices of the images labelled with the concept\n",
    "        '''\n",
    "        \n",
    "        if self.training_mode:\n",
    "            self.concept_idxs = self.training_dataframe.loc[self.training_dataframe[self.concept] == 1, 'image'].index.tolist()\n",
    "        else:\n",
    "            self.concept_idxs = self.test_dataframe.loc[self.test_dataframe[self.concept] == 1, 'image'].index.tolist()\n",
    "\n",
    "        return self.concept_idxs\n",
    "\n",
    "    def get_image_tensor(self, img_tensor):\n",
    "        \"\"\"\n",
    "        The function extracts the feature vector of an image from the 'avgpool' layer in a model\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.Tensor): a tensor of an image (likely returned by the torch.Dataset)\n",
    "    \n",
    "        Returns:\n",
    "            torch.Tensor: the vector of the image in the out_layer\n",
    "        \"\"\"\n",
    "\n",
    "        #Create a PyTorch Variable with the transformed image\n",
    "        img = Variable(img_tensor).unsqueeze(0)\n",
    "\n",
    "        # Create a vector of zeros that will hold the feature vector of the image. \n",
    "        # The 'avgpool' layer of the ResNet50 has an output size of 2048\n",
    "        my_embedding = torch.zeros(2048)\n",
    "\n",
    "        # Define a function that will copy the output of a layer\n",
    "        def copy_data(m, i, o):\n",
    "            my_embedding.copy_(o.data.squeeze())\n",
    "\n",
    "        # Attach that function to our selected layer\n",
    "        h = self.output_layer.register_forward_hook(copy_data)\n",
    "\n",
    "        # Run the model on our transformed image\n",
    "        self.n_network(img)\n",
    "\n",
    "        # Detach our copy function from the layer\n",
    "        h.remove()\n",
    "\n",
    "        # Return the feature vector\n",
    "        return my_embedding\n",
    "    \n",
    "    def make_index_tensor_dictionary(self, dictionary, dictionary_name, image_indices):\n",
    "        \"\"\"\n",
    "        Creates a dictionary with the row indices as key and the output tensor as value. The CNN is set to evaluation mode\n",
    "        after which every image is run through the CNN. The output tensor of the 'avgpool' layer is stored as value in the \n",
    "        dictionary. After every image has been passed through the network, the dictionary is saved on the local harddrive.\n",
    "        \n",
    "        Args:\n",
    "            dictionary: the dictionary which will be created\n",
    "            dicionary_name (string)\n",
    "            image_indices (list): list of image indices which to add to the dictionary\n",
    "        \n",
    "        Returns:\n",
    "            dictionary\n",
    "        \"\"\"\n",
    "    \n",
    "        if os.path.exists(os.path.join('../data/', dictionary_name)):\n",
    "            with open(os.path.join('../data/', dictionary_name), 'rb') as handle:\n",
    "                dictionary = pickle.load(handle) \n",
    "                \n",
    "            return dictionary\n",
    "\n",
    "        else:\n",
    "            if self.training_mode:\n",
    "                dataset = self.training_torch_dataset\n",
    "            else:\n",
    "                dataset = self.test_torch_dataset\n",
    "\n",
    "            # set the neural network to evaluation mode\n",
    "            self.n_network.eval()\n",
    "            \n",
    "            # loop through every index, push the image belonging to the index through the network and extract the tensor from\n",
    "            # the desired output layer\n",
    "            for idx in tqdm_notebook(image_indices):\n",
    "                sample = dataset[idx]\n",
    "                img_tensor = sample['image']\n",
    "                img_tensor = img_tensor.float()\n",
    "                vector_img = self.get_image_tensor(img_tensor)\n",
    "                dictionary[str(idx)] = vector_img\n",
    "\n",
    "            # write the dictionary to the local harddrive\n",
    "            with open(os.path.join('../data/', dictionary_name), 'wb') as handle:\n",
    "                pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            return dictionary\n",
    "        \n",
    "    def get_random_concept_images(self):\n",
    "        \n",
    "        '''Get a specified amount of random concept images from the total amount of images'''\n",
    "        \n",
    "        self.get_concept_indices()\n",
    "        if self.num_concept_imgs > len(self.concept_idxs):\n",
    "            raise ValueError ('The number of specified concept images (%d) is larger than the total amount of images labelled with the concept (%d)' \n",
    "                          % (self.num_concept_imgs, len(self.concept_idxs)))\n",
    "        else:\n",
    "            self.random_concept_idxs = random.sample(self.concept_idxs, self.num_concept_imgs)\n",
    "        \n",
    "    def get_random_counter_images(self):\n",
    "        '''Get a specified amount of random counter images which are not labelled with the concept'''\n",
    "        \n",
    "        self.get_concept_indices()\n",
    "        self.random_counter_idxs = [i for i in range(len(self.training_dataframe)) if i not in self.concept_idxs]\n",
    "        self.random_counter_idxs = random.sample(self.random_counter_idxs, self.num_counter_imgs)\n",
    "        \n",
    "    def make_random_counter_dictionary(self):\n",
    "        '''Create a dictionary containing the random counter images'''\n",
    "        \n",
    "        self.get_random_counter_images()\n",
    "        self.counter_dictionary = self.make_index_tensor_dictionary(self.counter_dictionary, self.counter_dictionary_name,\n",
    "                                                            self.random_counter_idxs)\n",
    "        \n",
    "    def train_lm(self):\n",
    "        '''Train a linear classifier between the concept images and the counter images'''\n",
    "        self.training_mode = True\n",
    "        self.get_concept_indices()\n",
    "        self.concept_dictionary = self.make_index_tensor_dictionary(self.concept_dictionary, \n",
    "                                                                    self.dictionary_name, \n",
    "                                                                    self.concept_idxs)\n",
    "        self.get_random_concept_images()\n",
    "        \n",
    "        self.make_random_counter_dictionary()\n",
    "        \n",
    "        concept_tensors = self.concept_dictionary[str(self.random_concept_idxs[0])].unsqueeze(0)\n",
    "        for i in range(1, self.num_concept_imgs):\n",
    "            temp_concept_tensor = self.concept_dictionary[str(self.random_concept_idxs[i])].unsqueeze(0)\n",
    "            concept_tensors = torch.cat((concept_tensors, temp_concept_tensor),0)\n",
    "            \n",
    "        counter_tensors = self.counter_dictionary[str(self.random_counter_idxs[0])].unsqueeze(0)\n",
    "        for i in range(1, self.num_counter_imgs):\n",
    "            temp_counter_tensor = self.counter_dictionary[str(self.random_counter_idxs[i])].unsqueeze(0)\n",
    "            counter_tensors = torch.cat((counter_tensors, temp_counter_tensor), 0)\n",
    "        \n",
    "        # concatenate all tensors to the same array\n",
    "        X = torch.cat((concept_tensors, counter_tensors), 0)\n",
    "        X = X.numpy()\n",
    "        \n",
    "        # create labels for the tensors\n",
    "        # 1 = concepts, 0 = not concept\n",
    "        y = np.ones(self.num_concept_imgs)\n",
    "        y = np.append(y, np.zeros(self.num_counter_imgs))\n",
    "            \n",
    "        self.lm = linear_model.SGDClassifier()\n",
    "        self.lm.fit(X, y)\n",
    "        \n",
    "        self.cav = self.lm.coef_\n",
    "        \n",
    "    def view_concept_images(self):\n",
    "        '''View the concept images used to train the linear classifier'''\n",
    "        \n",
    "        if len(self.concept_idxs) == 0:\n",
    "            raise ValueError ('No images have been selected yet')\n",
    "        \n",
    "        else:\n",
    "            %matplotlib inline\n",
    "            dim = math.floor(math.sqrt(len(self.concept_idxs)))\n",
    "\n",
    "            fig = plt.figure(figsize=(12,12))\n",
    "            ax = [fig.add_subplot(dim, dim, i+1) for i in range(dim**2)]\n",
    "\n",
    "            for idx, a in enumerate(ax):\n",
    "                img = plt.imread(os.path.join('../data/broden1_384/images/', \n",
    "                                              self.training_dataframe.loc[self.concept_idxs[idx], 'image']))\n",
    "                a.axis('off')\n",
    "                a.imshow(img)\n",
    "\n",
    "            fig.subplots_adjust(wspace=0, hspace=0)\n",
    "            plt.show()\n",
    "            \n",
    "    def view_counter_images(self):\n",
    "        '''View the counter images used to train the linear classifier''' \n",
    "        \n",
    "        if len(self.random_counter_idxs) == 0:\n",
    "            raise ValueError ('No counter images have been selected')\n",
    "            \n",
    "        else:\n",
    "            %matplotlib inline\n",
    "            dim = math.floor(math.sqrt(len(self.random_counter_idxs)))\n",
    "\n",
    "            fig = plt.figure(figsize=(12,12))\n",
    "            ax = [fig.add_subplot(dim, dim, i+1) for i in range(dim**2)]\n",
    "\n",
    "            for idx, a in enumerate(ax):\n",
    "                img = plt.imread(os.path.join('../data/broden1_384/images/', \n",
    "                                              training_data.loc[self.random_counter_idxs[idx], 'image']))\n",
    "                a.axis('off')\n",
    "                a.imshow(img)\n",
    "\n",
    "            fig.subplots_adjust(wspace=0, hspace=0)\n",
    "            plt.show()\n",
    "    \n",
    "    def create_test_data(self):\n",
    "        self.training_mode = False\n",
    "        test_scene_dictionary_name = 'test_scene_dictionary.pickle'\n",
    "        \n",
    "        for scene in self.scene_concepts:\n",
    "            scene_idx = self.test_dataframe.loc[self.test_dataframe[scene] == 1, 'image'].index.tolist()\n",
    "            if len(scene_idx) > 50:\n",
    "                scene_idx = random.sample(scene_idx, 50)\n",
    "\n",
    "            self.test_scene_dict[scene] = scene_idx\n",
    "            list(map(lambda x: self.test_scene_idxs.append(x), scene_idx))\n",
    "        \n",
    "    \n",
    "        self.test_scene_vector_dictionary = self.make_index_tensor_dictionary(self.test_scene_vector_dictionary, \n",
    "                                                                              test_scene_dictionary_name,\n",
    "                                                                              self.test_scene_idxs)\n",
    "        # if the dictionary is loaded from the harddrive, the indices might change due to the random selection of images\n",
    "        # by reassigning it to the keys of the dictionary (which equals the indices), the indices will match with the \n",
    "        # dictionary\n",
    "        self.test_scene_idxs = list(self.test_scene_vector_dictionary.keys())\n",
    "            \n",
    "        \n",
    "        self.X_test = self.test_scene_vector_dictionary[str(self.test_scene_idxs[0])].unsqueeze(0)\n",
    "        for i in range(1, len(self.test_scene_idxs)):\n",
    "            temp_vector = self.test_scene_vector_dictionary[self.test_scene_idxs[i]].unsqueeze(0)\n",
    "            self.X_test = torch.cat((self.X_test, temp_vector),0)\n",
    "\n",
    "        self.X_test = self.X_test.numpy()\n",
    "        \n",
    "        #Create labels for test data   \n",
    "        start_idx = 0\n",
    "        for key in self.scene_concepts:\n",
    "            if key == self.concept:\n",
    "                break\n",
    "            else:\n",
    "                start_idx += len(self.test_scene_dict[key])\n",
    "\n",
    "        end_idx = start_idx + len(self.test_scene_dict[self.concept])+1\n",
    "\n",
    "        #create the right labels on the right tensors\n",
    "        self.y_test = np.zeros(len(self.test_scene_idxs))\n",
    "        self.y_test[start_idx:end_idx] = 1\n",
    "            \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "hayfield = TorchCAV('hayfield-s', 10, 100, basenet, out_layer, train_broden_dataset, test_broden_dataset,\n",
    "                          training_data, test_data, scene_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "hayfield.create_test_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hayfield.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9d1e4b7ceb4af48f68100895e9f722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=97), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae01379fec2488b92e3b72c6be1170c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mountain_snowy = TorchCAV('mountain_snowy-s', 50, 200, basenet, out_layer, train_broden_dataset, test_broden_dataset,\n",
    "                         training_data, test_data)\n",
    "mountain_snowy.train_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
