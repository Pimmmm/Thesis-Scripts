{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-04dc92e72dab>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-04dc92e72dab>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    def GetConceptIndices(self, self.concept, self.training):\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class TorchCAV(object):\n",
    "    \n",
    "    def __init__(self, concept, n_network, output_layer, torch_dataset, training_dataframe, test_dataframe):\n",
    "\n",
    "        self.concept = concept\n",
    "        self.n_network = n_network\n",
    "        self.output_layer = output_layer\n",
    "        self.torch_dataset = torch_dataset\n",
    "        self.training_dataframe = training_dataframe\n",
    "        self.test_dataframe = test_dataframe\n",
    "        self.training = True\n",
    "\n",
    "        if self.concept not in list(self.training_dataframe):\n",
    "            raise ValueError ('%s is not present in the dataset' %self.concept)\n",
    "\n",
    "    def GetConceptIndices(self, self.concept, self.training):\n",
    "        if training:\n",
    "            data = self.training_dataframe\n",
    "        else:\n",
    "            data = self.test_dataframe\n",
    "\n",
    "        self.concept_idxs = data.loc[data[self.concept] == 1, 'image'].index.tolist()\n",
    "        return(self.concept_idxs)\n",
    "\n",
    "    def get_image_tensor(self, self.img_tensor, self.n_network, self.output_layer):\n",
    "        \"\"\"\n",
    "        The function extracts the feature vector of an image from the 'avgpool' layer in a model\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.Tensor): a tensor of an image\n",
    "            self.n_network (torchvision.model): a Neural Network or Convolutional NN\n",
    "            self.out_layer: a layer of the nn, from which the output will be extracted\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the vector of the image in the out_layer\n",
    "        \"\"\"\n",
    "\n",
    "        #Create a PyTorch Variable with the transformed image\n",
    "        img = Variable(img_tensor).unsqueeze(0)\n",
    "\n",
    "        # Create a vector of zeros that will hold the feature vector of the image. \n",
    "        # The 'avgpool' layer of the ResNet50 has an output size of 2048\n",
    "        my_embedding = torch.zeros(2048)\n",
    "\n",
    "        # Define a function that will copy the output of a layer\n",
    "        def copy_data(m, i, o):\n",
    "            my_embedding.copy_(o.data.squeeze())\n",
    "\n",
    "        # Attach that function to our selected layer\n",
    "        h = self.output_layer.register_forward_hook(copy_data)\n",
    "\n",
    "        # Run the model on our transformed image\n",
    "        self.n_network(img)\n",
    "\n",
    "        # Detach our copy function from the layer\n",
    "        h.remove()\n",
    "\n",
    "        # Return the feature vector\n",
    "        return my_embedding\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
